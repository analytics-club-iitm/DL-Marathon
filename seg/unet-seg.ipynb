{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open All Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/analytics-club-iitm/DL-Marathon/blob/main/seg/unet-seg.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "SMOOTH = 1e-6\n",
    "class SemSegData(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self._init_dataset()\n",
    "        \n",
    "    def _init_dataset(self):\n",
    "\n",
    "        self.img_list = glob(self.root_dir + '/original_images/*')\n",
    "        self.mask_list = glob(self.root_dir + '/label_images_semantic/*')\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img = self.img_list[index]\n",
    "        mask = self.mask_list[index]\n",
    "\n",
    "        img = (torch.tensor(np.array(Image.open(img).resize((128,128)))).permute(2,0,1) )/255\n",
    "        \n",
    "        \n",
    "        mask = torch.unsqueeze(torch.tensor(np.array(Image.open(mask).resize((128,128),Image.NEAREST))),0)\n",
    "        \n",
    "            \n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "#3x3 convolution for the U-Net Architecture\n",
    "class conv3x3(nn.Module):\n",
    "    def __init__(self, k,p,s,in_channels = 256, out_channels = 256, activation = nn.ReLU()):\n",
    "        super(conv3x3,self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Conv2d(in_channels, out_channels, kernel_size = k, padding = p,  stride = s,bias=False)\n",
    "        self.layer2 = nn.BatchNorm2d(out_channels)\n",
    "        self.layer3 = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "#Double Conv definition which is the biggest independent unit of the architecture\n",
    "class Double_Conv(nn.Module):\n",
    "    def __init__(self, in_channels = 256, out_channels = 256, activation = nn.ReLU()):\n",
    "        super(Double_Conv,self).__init__()\n",
    "\n",
    "        self.layer1 = conv3x3(3,1,1,in_channels, out_channels)\n",
    "        \n",
    "        self.layer2 = conv3x3(3,1,1,out_channels, out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "#Downsampling module for the U-Net\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels , out_channels ):\n",
    "        super(Down,self).__init__()\n",
    "        '''\n",
    "        #self.layer1 = nn.MaxPool2d(2)\n",
    "        self.model = []\n",
    "        self.conv1 = conv3x3(3,1,1,in_channels,out_channels)\n",
    "        \n",
    "        self.model = nn.ModuleList([Double_Conv(out_channels, out_channels)]+[Double_Conv(out_channels*2, out_channels) for i in range(2)])\n",
    "\n",
    "        self.conv = nn.Sequential(nn.Conv2d(out_channels*4,out_channels,3,1,1),\n",
    "                                  nn.BatchNorm2d(out_channels),\n",
    "                                  nn.ReLU())\n",
    "        '''\n",
    "        self.model = nn.Sequential(Double_Conv(in_channels, out_channels),Double_Conv(out_channels, out_channels) )\n",
    "        self.down = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "\n",
    "        return self.down(x),x\n",
    "\n",
    "#Upsampling module for the U-Net\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, k,p,s,in_channels, mid,out,activation = nn.ReLU()):\n",
    "        super(Up,self).__init__()\n",
    "\n",
    "        #self.layer1 = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n",
    "        self.layer2 = nn.Sequential(nn.ConvTranspose2d(in_channels, mid, kernel_size = k,padding =p,stride=s,bias=False),\n",
    "                                    nn.BatchNorm2d(mid),\n",
    "                                    nn.ReLU()\n",
    "                                    )\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(mid*2, out, kernel_size = 3,padding =1,stride=1,bias=False),\n",
    "                                    nn.BatchNorm2d(out),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(out, out, kernel_size = 3,padding =1,stride=1,bias=False),\n",
    "                                    nn.BatchNorm2d(out),\n",
    "                                    nn.ReLU(),\n",
    "                                    \n",
    "                                    )\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x,inp):\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = torch.cat((x,inp),1)\n",
    "        \n",
    "        out = self.layer3(x)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "\n",
    "#The U-net Model\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, img_dim = 572, activation = nn.ReLU()):\n",
    "        super(Unet,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3,64,3,padding=1,bias=False),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(64,64,3,padding=1,bias=False),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU()\n",
    "                                    )\n",
    "                                    \n",
    "        self.layer2 = nn.ModuleList( [  Down(64,64), # 128\n",
    "                                        Down(64,128), # 64\n",
    "                                        Down(128,256), # 32\n",
    "                                        Down(256,512), # 16\n",
    "                                        Down(512,512), # 8\n",
    "                                        Down(512,512), # 4\n",
    "                                        ]\n",
    "                                    )\n",
    "        self.conv = conv3x3(3,1,1,512,512)\n",
    "        self.layer8 = Up(4,1,2,512, 512,512) # 4\n",
    "        self.layer9 = Up(4,1,2,512, 512,512) # 8\n",
    "        self.layer10 = Up(4,1,2,512, 512,256) # 16\n",
    "        self.layer11 = Up(4,1,2,256,256, 128) # 32\n",
    "        self.layer12 = Up(4,1,2,128,128,64) # 64\n",
    "        self.layer13 = Up(4,1,2,64,64,64) # 128\n",
    "        self.layer14 = nn.Conv2d(64,24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        out = []\n",
    "        for i in self.layer2:\n",
    "            \n",
    "            x,o = i(x)\n",
    "            \n",
    "            out.append(o)\n",
    "        \n",
    "        x = self.conv(x) \n",
    "        x = self.layer8(x,out[-1])\n",
    "        \n",
    "        x = self.layer9(x,out[-2])\n",
    "        \n",
    "        x = self.layer10(x,out[-3])\n",
    "        \n",
    "        x = self.layer11(x,out[-4])\n",
    "        \n",
    "        x = self.layer12(x,out[-5])\n",
    "        \n",
    "        x = self.layer13(x,out[-6])\n",
    "        x = self.layer14(x)\n",
    "\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import datasets\n",
    "import random\n",
    "import numpy as np \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train():\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_epochs = 200\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "\n",
    "\n",
    "    def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n",
    "        ''' \n",
    "        Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "        Assumes the `channels_last` format.\n",
    "    \n",
    "        # Arguments\n",
    "            y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n",
    "            y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n",
    "            epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "        \n",
    "        # References\n",
    "            V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation \n",
    "            https://arxiv.org/abs/1606.04797\n",
    "            More details on Dice loss formulation \n",
    "            https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "            \n",
    "            Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "        '''\n",
    "        \n",
    "        # skip the batch and class axis for calculating Dice score\n",
    "        mask = torch.zeros((y_true.size(0),24,128,128)).to(device)\n",
    "        outs = torch.softmax(y_pred,1)\n",
    "        mask.permute(0,2,3,1).contiguous().view(-1,24)[torch.arange(y_true.size(0)*128*128),y_true.view(-1)] = 1\n",
    "        \n",
    "        numerator = 2. * (outs *mask).sum((2,3))\n",
    "        denominator = torch.sum(torch.square(outs) + torch.square(mask),(2,3))\n",
    "        \n",
    "        return torch.mean(1 - torch.mean((numerator + epsilon) / (denominator + epsilon),1)) \n",
    "\n",
    "    dataset = SemSegData(root_dir='E:/DL-Week/archive/dataset/semantic_drone_dataset')\n",
    "    data_len = len(dataset)\n",
    "\n",
    "    trainset, valset = random_split(dataset, [int(0.8*data_len), (data_len - int(0.8*data_len))])\n",
    "\n",
    "    trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle = True)\n",
    "    valloader = DataLoader(dataset=valset, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "    model = Unet().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best = float(\"inf\")\n",
    "    train_step = 0\n",
    "    val_step = 0\n",
    "    lr_lbmd = lambda e: max(0.7**(e//20), 0.00001/0.001)\n",
    "    \n",
    "    optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        \n",
    "        model.load_state_dict(torch.load('best4.ckpt'))\n",
    "        optimiser.load_state_dict(torch.load(\"optim_best4.ckpt\"))\n",
    "        print(\"model_loaded\")\n",
    "        \n",
    "\n",
    "    except:\n",
    "        optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        print(\"save not found\")\n",
    "    \n",
    "        \n",
    "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimiser, lr_lbmd)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(\"---------------\")\n",
    "        model.train()\n",
    "        loss_cntr = []\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        for indx, (img, mask) in enumerate(trainloader):\n",
    "            \n",
    "            img, mask = img.to(device), mask.to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            mask = mask.long()\n",
    "            \n",
    "            outputs = model(img)\n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs.permute(0,2,3,1).contiguous().view(-1,24),mask.view(-1,))\n",
    "            loss2 = soft_dice_loss(mask,outputs)\n",
    "            t_loss = loss + loss2\n",
    "            optimiser.zero_grad()\n",
    "            t_loss.backward(retain_graph = False)\n",
    "            optimiser.step()\n",
    "\n",
    "\n",
    "            loss_cntr.append(loss.item())\n",
    "            #wandb.log({\"train loss\": loss.item(), \"train step\": train_step})\n",
    "            train_step += 1\n",
    "\n",
    "            datasets.progress_bar(progress=indx/len(trainloader), status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
    "            writer.add_scalar(\"Loss/train\", loss, train_step)\n",
    "            \n",
    "            \n",
    "        datasets.progress_bar(progress=1, status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        loss_cntr = []\n",
    "        model.eval()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            for indx, (img2, mask2) in enumerate(valloader):\n",
    "                \n",
    "                img2, mask2 = img2.to(device), mask2.to(device)\n",
    "                #out2 = model_base(img2)\n",
    "                out2 = model(img2)\n",
    "                \n",
    "                \n",
    "                    \n",
    "                    \n",
    "\n",
    "                mask2 = mask2.long()\n",
    "                \n",
    "            \n",
    "                \n",
    "                loss = criterion(out2.permute(0,2,3,1).contiguous().view(-1,24),mask2.view(-1,))\n",
    "                loss2 = soft_dice_loss(mask2,out2)\n",
    "            \n",
    "                loss = loss + loss2        \n",
    "                \n",
    "                loss_cntr.append(loss.item())\n",
    "                val_step += 1\n",
    "\n",
    "                datasets.progress_bar(progress=indx/len(valloader), status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
    "                writer.add_scalar(\"Loss/val\", loss, val_step)\n",
    "            datasets.progress_bar(progress=1, status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
    "\n",
    "        #wandb.log({\"val epoch\": epoch, \"val loss\": np.mean(loss_cntr)})\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if np.mean(loss_cntr) < best:\n",
    "            best = np.mean(loss_cntr)\n",
    "            torch.save(model.state_dict(), \"best5.ckpt\")\n",
    "            torch.save(optimiser.state_dict(), \"optim_best5.ckpt\")\n",
    "            print(f\"current best: {round(best, 4)}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"last.ckpt\")\n",
    "        torch.save(optimiser.state_dict(), \"optim_last.ckpt\")\n",
    "        lr_scheduler.step()\n",
    "        a_mask = torch.argmax(out2,1,keepdim=True).long()\n",
    "        grid = torchvision.utils.make_grid(a_mask)\n",
    "        ref_grid = torchvision.utils.make_grid(mask2)\n",
    "        grid2 = torchvision.utils.make_grid(img2)\n",
    "        \n",
    "        \n",
    "        writer.add_image(f\"img\", grid2,val_step)\n",
    "        writer.add_image(f\"pred\",grid,val_step)\n",
    "        writer.add_image(f\"mask\", ref_grid,val_step)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
